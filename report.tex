\documentclass[12pt, conference, final, a4paper, onecolumn, compsoc]{IEEEtran}
% Font size onecolumn or twocolumn Use draft for notes or final for no spacing

% Includegraphics
\usepackage{graphicx}
% Coloured code
\usepackage{color}
% Code listings
\usepackage{listings} \lstset{ basicstyle=\small, showstringspaces=false,
  % commentstyle=\itshape\small\color{gray}, keywordstyle=\color{cyan}\bfseries,
  % numberstyle=\color{gray}, numbers=left, numbersep=8pt
}
% Paragraph spacing - very good!
\usepackage{parskip}
% Bibliography
\usepackage{natbib}
% Figure positions
\usepackage{float}
% Wrapping figures
\usepackage{wrapfig}
% Wrapping URLs
\usepackage[hyphens]{url}
% Changes size of font in captions
\usepackage{caption}
\DeclareCaptionFont{captionfontsize}{\fontsize{12}{14}\selectfont}
\captionsetup{font=captionfontsize}

\begin{document}


\title{Malware Detection within Object Storage} \author{Author: Matthew
  Battagel, Supervisor: Theodoros Spyridopoulos} \markboth{Cardiff University -
  CM3203 - Final Report}{}
\maketitle{}

\subsection*{Acknowledgments - }
% Remove section from TOC TODO TOC: Only showing section
\addtocontents{toc}{\protect\setcounter{tocdepth}{-1}}

I would like to extend my sincere gratitude to my supervisor Theo, my colleague
Harry, friends, family, and Lo for their unwavering support and encouragement
during my project. Their combined expertise and guidance provided were critical
in the shaping and execution of the project. I am truly grateful to all of them
for their contributions.

\bigskip

\begin{abstract}
  Lorem Ipsum
\end{abstract}

\pagebreak

% Problem and background Understanding of the problem and the aims and
% objectives of the project Awareness of the background of the problem

% Detailed analysis of the problem, suitability of approach towards solving the
% problem Solution to the problem Approach and design Solution, implementation
% Use of and justification for appropriate tools/methods

% Evaluation Testing and validation Critical appraisal of results

% Achievement of agreed overall deliverables given in the initial plan for the
% final report (or a justified modification of these) Communication and project
% management skills Written communication skills Project planning, control and
% reflection Interaction and work with the supervisor


% Contents
\tableofcontents{}


\section{Introduction}
\subsection*{Overview}
\paragraph{}
% Data growth and object storage
The exponential growth of data generation has made data storage an increasingly
important aspect for both individuals and organizations alike. Object storage
has emerged as a promising solution due to its ability to store vast amounts of
unstructured data in a cost-effective and scalable manner. Unlike traditional
storage techniques, object storage stores data as objects with related metadata
and unique identifiers, allowing for efficient and cheap storage within buckets.

% TODO Overview: Re-word last part

% Market and competition
One of the most widely used object storage platforms is Amazon S3, which
provides a highly scalable and reliable solution for storing data. However, an
open-source alternative called MinIO has emerged as a promising contender,
providing similar features to Amazon S3 while giving customers greater control
over their data. MinIO is written in Go and is available for free under the
Apache License 3.0 or, for commercial and enterprise purposes, at a reduced cost
compared to Amazon S3 \citep{minio-pricing}. MinIO offers a wide range of
features, including high performance, data replication, encryption and erasure
coding \citep{minio}. Most importantly, MinIO is designed to scale out
horizontally to ensure that it can handle the demands of large-scale
applications.

% Scalability
Scalability is made simple by allowing multiple types of hardware platforms to
work together in separate nodes each with their own compute and storage. This is
extremely attractive for customers who want to utilises their existing hardware
without being tied down to a specific provider. This also applies for customers
looking to migrate their data from Amazon S3 to cheaper solution without
compromising on the high performance, reliability and scalability of the S3
platform.

% Downsides
While MinIO is a great alternative to Amazon S3, it does not offer any form of
malware detection integration. This could put customers off from choosing MinIO
as a viable platform to migrate to from Amazon S3 or leave existing users data
vulnerable to malware attacks. This project aims to address this issue by
integrating a malware detection system into MinIO. An important goal for the is
to negatively impact the scalability or performance as little as possible so
that MinIO is still an effective alternative to Amazon S3.

\subsection*{Motivation} % Why am I trying to add malware detection for object storage?
% TODO Motivation: Can I say about HPE? Or shall I say its for the good of the
% world

Due to the high amount of unstructured data expected to be both written and read
to the object store, there are increased risk of encountering malicious files.
Therefore malware detection within object storage is crucial in modern cloud
storage scenarios. Most popular off-the-shelf object storage platforms, such as
AWS, already have integrated third-party antivirus software, such as ClamAV and
Sophos \citep{amazon-md}, to mitigate security risks. MinIO on the other hand is
vulnerable to malware attacks as it currently does not have any native antivirus
integration. This forces customers who require complete virus protection to
either not use MinIO or to use potentially costly third-party software. As
antivirus scanning is inherently resource intensive, if the software is
integrated incorrectly, it could reduce the ability for the storage solution to
scale horizontally which negates one of the major benefits of object storage.
The purpose of this project is to implement malware detection within MinIO while
being mindful to not impact the scalability or performance of the platform.

\subsection*{Project Aims} % What the project aims to achieve

% TODO Project Aims: This is more about personal goals for the project.

\paragraph{}
The primary objective of this project is to integrate malware detection into
MinIO without compromising the platform's scalability or performance. Specific
targets are elaborated in the specification section.

From a personal standpoint, this project serves as an opportunity to enhance my
knowledge of the Go programming language and the MinIO platform. Additionally,
it aims to expand my experience in designing and implementing production-ready
solutions within a micro-service environment. Lastly, this project seeks to
develop my proficiency in managing time, scope, and resources for large-scale
projects.

\section{Background}

% background should be more previous research material etc. Include competition
% and potential software to use e.g. ClamAV, MinIO. Include the first things you
% find when googling the same topic as diss.

% Where to put background info on object storage and malware? Not sure
%

% Insert background material
%

\subsection*{Amazon S3 Malware Detection} % How does amazon do it?
\paragraph{}

As MinIO's largest competitor, this project draws a lot of inspiration from
Amazon S3s integrated malware detection blog page \citep{amazon-md}. The blog
explains Amazons current approach for managing malware detection within their
service. Amazon S3 uses a combination of ClamAV and Sophos as their third-party
scanning engines due to their out-of-the-box nature. Amazon then gives you the
option to use either of these engines or both. The blog goes on to describe the
three main interaction mechanisms that Amazon S3 uses to flag files for
scanning. Firstly, an API endpoint would be provided to handle all uploads. This
forms a queue of uploads which are then scanned before entering the bucket.
Next, event-driven scanning is used keep track of all regular file uploads. The
antivirus will then scan each file after they have been written to the bucket.
Finally, retro-driven scanning is used to scan all existing files within the
bucket. The user then has the flexibility to define what types of files should
be scanned including defining time windows. This blog has given some useful
methodologies of how to keeping track of both incoming and previously scanned
files. Creating a system that can match these methods is important for offering
a matching level of scalability and security within MinIO.

% Talk about standard flow and two bucket flow - useful in designing

% Amazon use stub files

% \subsection*{} % How does signature detection work?


% \subsection*{} % Best ways of implementing AV into a micro-service?


% \subsection*{} % How does ClamAV work? What is clamd

% Malware Artificial Intelligence Object Storage Context

\section{Specification}

The purpose of the project specification is to guide the project towards
achieving the goals outlined in the previous section. The specification is
divided into three main categories: functional requirements, non-functional
requirements, and constraints.

The MoSCoW method is employed to prioritize the projects requirements. It is a
prioritization technique utilized in business analysis and software development
to establish a shared understanding among stakeholders regarding the
significance assigned to the delivery of each requirement
\citep{moscow-prioritization}. The acronym stands for:

\begin{itemize}
  \item Must have: Critical for the project's success and must be included in
        the final solution.
  \item Should have: Important but not absolutely necessary for the project's
        success. They should be included if possible, but the project can
        proceed without them.
  \item Could have: Desirable but not essential. They can be included if time
        and resources permit but can be left out without impacting the project's
        success.
  \item Won't have: Not necessary for the current project and will not be
        included. They may be considered for future development.
\end{itemize}

\subsection*{Functional Requirements}
\paragraph{}

Functional requirements are provided in the table \ref{tab:functional-reqs}. The
requirement is given with a MoSCoW priority and a description to justify the
priority.

\begin{table}[H]
  \centering
  \begin{tabular}{|p{0.4\textwidth}|p{0.5\textwidth}|l|} \hline
    \textbf{Requirement} & \textbf{Description} & \textbf{Priority} \\ \hline
Provide a way of detecting the latest uploads to the object store & Key part of
automating scans & Must \\ \hline Record the results of the malware detection
within the object store & Some feedback from the scan must be provided to the
user. & Must \\ \hline Provide the ability to measure various metrics & Metric
collection is very attractive for satisfying production-ready status. & Must \\
\hline Scale alongside MinIO to ensure that it does not bottleneck the object
store at high loads & To handle maximum throughput, the solution should scale
alongside MinIO to ensure it is production-ready. & Must \\ \hline Provision for
future expansion and ongoing maintenance & Having the ability to build on-top of
the project will increase the attractiveness to users looking to migrate from
                                           AWS & Must \\ \hline
    \hline

    Have a high level of customisability to allow for different use cases &
Gives flexibility to the user but is not a requirement for proof of concept &
Should \\ \hline Allow for efficient and transparent debugging in the event of
    failure & Creating a robust product mitigates the priority of debugging. & Should \\ \hline

Add more complex metrics - Histogram, Gauges, etc & Additional metrics are more
                                                    beneficial in a production
                                                    environment but unnecessary
                                                    for size of project scope. & Should \\ \hline
    \hline
    Provide the choice of multiple antivirus engines & not necessary for the
proof of concept as it requires too much time. & Could \\ \hline
    Provide a warning when under ``delete'' cleanup policy & not necessary for the
proof-of-concept as it requires too much time. & Could \\ \hline
    Automatically event queue to bucket notifications & Can be completed
                                                        manually with
                                                        instructions instead.
                                                        Removes customisability
                                                        from the user if done automatically. & Could \\ \hline
     Supply the MinIO policies the solution will use e.g. ``put'' and ``get''. &
                                                                                 MinIO
                                                                                 denies
                                                by default and therefore
                                                                                 operations
                                                                                 could
                                                fail when MinIO is not run with
                                                                                 admin
                                                                                 permissions.
                                                & Could \\ \hline
    \hline

    Prevent the downloading of files if flagged as infected & Is covered by
cleanup policies. Would also require GUI changes. & Wont \\ \hline Protection
from ``inside man'' attacks & The system will be maintained by an admin. The
admin could attempt to compromise the system. Protection against this is
significantly harder without reducing extensibility. & Wont \\ \hline Provide
multiple types of scanners e.g. hash or server based AV scanner & These would
have different workloads and requirements. Not achievable in project scope. &
                                                                              Wont \\ \hline
  \end{tabular}
  \caption{Functional Requirements}
  \label{tab:functional-reqs}
\end{table}

% TODO Functional Requirements: End with a sentence?

% TODO Functional Requirements: Say about we need to keep up so that people are
% not waiting and the system does not get behind. Maybe use metrics to show how
% we DO keep up?

\subsection*{Non-Functional Requirements}
\paragraph{}

\begin{table}[H]
  \centering
  \begin{tabular}{|l|p{0.7\textwidth}|l|}
    \hline
    \textbf{Requirement} & \textbf{Description} & \textbf{Priority} \\ \hline
    Speed & The solution must be able to keep up with the rate of uploads made to MinIO. This can be measured by comparing the time difference between uploading an object to MinIO and the object being scanned and tagged & Must \\ \hline
    Availability & Over a long period of time, the solution must be able to handle all requests. This can be measured by comparing the number of requests made to the number of requests completed over a large time frame & Must \\ \hline
    Capacity & The solution must be able to handle the maximum number of simultaneous requests that MinIO can handle. This can be measured by monitoring the amount of cache used by the solution under load & Should \\ \hline
    Reliability & 100\% of the files uploaded to MinIO must go through the scanning process. The recorded metrics can be used to compare MinIO uploads with the number of objects scanned. It is worth noting that checking the clean and infected results add to the total sum of scanned objects & High \\ \hline
    Usability & Future additions, maintenance, and debugging must be as simple as possible. This requirement is more subjective and therefore an explanation of how this has been achieved will be discussed in the implementation section & Should \\ \hline
    Security & Ensure the solution follows best practices for security, including data protection and secure communication between components & Must \\ \hline
    Privacy & The solution must respect user privacy by adhering to privacy laws and regulations, as well as minimizing data collection and ensuring data is stored securely & Must \\ \hline
  \end{tabular}
  \caption{Non-Functional Requirements}
  \label{tab:non_functional_requirements}
\end{table}

\subsection*{Constraints}
\paragraph{}

% TODO Constraints: Include heatmap?

The constraints are the limitations that the solution must adhere to. The main
constraint of the project is the strict time limit given to the project. There
are a total of 12 weeks to achieve a production ready product which will greatly
limit the scope of the project. This means accurately prioritising the features
that are most important to the project while also balancing the time spent to
implement them. The second constraint is the limited resources available to the
project.

Another constraint of the project is that all the external software used must be
open source / available for commercial use under license or fee. This is to
ensure that the project is legally viable if the solution was to be used
commercially.

The final constraint is that my own knowledge and experience will increase the
average time taken to implement milestones. This is due to the fact that extra
time must be provided to learn any new technology. Any errors made will also
cause unexpected delays to the project therefore ease of use, documentation and
community support will be taken into account when choosing technologies.

% TODO Constraints: Make sure I mitigate these somewhere

\section{Architecture}

\paragraph{}
Choosing the correct architecture for the project is critical for ensuring that
the solution is scalable, performant and maintainable. Given the specification
above, various potential architectures can be created and evaluated based my own
thoughts and from reading the background material. The best candidate design
will then be chosen based on which candidate design satisfies the most
requirement with as little compromises as possible. Thought will also be given
to which architecture fits within the constraints of the project.

\subsubsection*{Project Naming}
\paragraph{}
Due to the defined functionality of the solution, an suitable name can be
chosen. ``Aegis'' is the name of the shield that Zeus used to protect himself in
Greek mythology and is also a noun synonymous with protection. This is
appropriate given that the solution is designed to protect the MinIO object
store from malware. Aegis will be used synonymously with the solution throughout
the rest of the report.

\subsection*{Candidate Design 1 - Post-Write}
\paragraph{}

The first candidate design makes use of the performance benefits of MinIO by
allowing puts to be initially written to the bucket without being scanned. The
design then uses a event queue compatible with MinIO to keep track of all the
files that have been uploaded. The queue is then used to trigger a scan of the
file once an antivirus is available. The candidate design is shown in figure
\ref{fig:postWriteArch}.

\begin{wrapfigure}{r}{0.45\textwidth}
  \includegraphics[scale=.4]{diagrams/post-write.png}
  \caption{Post-Write Architecture}
  \label{fig:postWriteArch}
\end{wrapfigure}

This design has many benefits over other potential implementations. Firstly, it
uses the storage provided by MinIO to store all incoming files without having to
manage a separate storage solution. This removes a lot of complexity from the
solution by not having to account for a number of failure conditions that could
occur with a high availability, production ready storage solution. For example,
the solution would not be responsible for handling partial writes, loss of data,
or data corruption. Removing this responsibility allows the solution to focus on
the core functionality of the project, the scanning of files, which is essential
for keeping the project within the time constraints.

Secondly, the design also makes use of the integrated event queue provided by
MinIO. This again removes responsibility from the solution by differing the
scalability and reliability requirements of an event queue to MinIO.

Lastly, having Aegis dispatch the files to a scalable number of antivirus
scanners allows the solution to scale to meet the demands of the system. This
meets a key requirement as the solution is expected to have the capacity for a
large number of operations. This method does require the use of a load balancer
to effectively distribute the load across the available antivirus scanners.

The candidate design also has a number of drawbacks. Firstly, the design still
requires a small about of cache to temporarily store the object when it is being
dispatched to the antivirus. Provisioning of this cache has to be large enough
to handle the largest file possible to be uploaded to the object store. In
reality, this cache would be provisioned even larger to allow for the temporary
storage of multiple objects while multiple scans are being performed
asynchronously. In addition, the cache needs to be large enough to ensure that
the system does not become overwhelmed by the number of objects being scanned as
the system scales. This is a minor issue as store capacity is cheap and the
provisioning of the cache easy to scale up. Additionally, a higher priority can
be given to scaling up and out antivirus scanners to ensure that the smallest
number of files are being cached, while bring scanned, at any point.

% TODO Candidate Design 1: Do they know what get/put is? Do they know
% distributed storage topologies?

The second drawback is that, for each event, Aegis makes a get request for the
object to be scanned. This effectively doubles the number of requests made to
the object store. This also means that Aegis must have the ability to get any
file expected to be scanned and therefore must have access to the whole storage
network. The impact of this drawback is mitigated as the solution is expected to
be deployed on the same network as the object store which should reduce the
latency of each request made by Aegis. However, this still leaves MinIO to
handle twice as many requests with the performance loss being noticed mainly on
more distributed storage topologies.

Thirdly, the candidate design only allows for a single Aegis instance to
dispatch all incoming objects to available scanners. This is a potential
bottleneck for the system as this instance could become overwhelmed by the
number of requests it is receiving. This is a minor issue as the dispatching of
objects to scanners is not as performance intensive as other areas of the
solution, such as the actual scanning, and therefore it is not expected to be a
major bottleneck.

Lastly, any object uploaded to the store will have a certain period of time
where it remains unchecked. In this time, the user could potentially download an
unscanned object or the object could cause harm to the store before it is
detected. Although the handling of infected objects is out of scope, in an
actual implementation of the solution, the user could be made unable to download
unscanned objects until they have been scanned.

% TODO Candidate Design 1: AWS does this with S3 events 'standard flow'

\subsection*{Candidate Design 2 - Upload Queue}
\paragraph{}

% TODO Candidate Design 2: More explanation here?
This candidate design created a wrapper around MinIO that the user interacts
with instead of MinIO. This means that all puts go through Aegis before being
uploaded to the object store. The candidate design is shown in figure
\ref{fig:uploadQueueArch}.

\begin{wrapfigure}{r}{0.45\textwidth}
  \centering \includegraphics[scale=.4]{diagrams/upload-queue.png}
  \caption{Upload Queue Architecture}
  \label{fig:uploadQueueArch}
\end{wrapfigure}

The main benefit of this design is that the user interacts only with Aegis when
uploading files. This means that all incoming files can be stored within a
temporary storage before ever entering the object store. This offers the best
protection against malicious files as the user cannot ever download an unscanned
or infected file as it is never uploaded to the object store. Infected files can
then either be deleted or moved to a separate quarantine store for analysis.

This candidate designs main advantage also comes with a major drawback. This
design requires Aegis to handle the full throughput of all the puts to the
system. Aegis then has the full responsibility of being available to all puts
and, in a failure scenario, to handle the recovery of the system. Additionally,
the cache provisioned must be large enough to handle the largest files at
maximum throughput with extra room for unexpected delays. This negatively
affects the scope of the project by requiring the solution to prioritise
features that are already covered by MinIO.

% TODO Candidate Design 2: Cover mitigation

% TODO Candidate Design 2: is this already covered?
Because MinIO is dependent on Aegis to handle the puts, MinIO must wait to be
passed incoming objects sequentially after Aegis has finished processing the
previous object. This removes the potential for aggregate performance where

% TODO Candidate Design 2: Talk about how AWS uses this method 'Two Bucket
% System Flow'

\subsection*{Candidate Design 3 - Write Interception}
\paragraph{}

% TODO Candidate Design 3: Edit diagram to show quarantine?

Candidate Design three is very similar to the second candidate design, however,
instead of wrapping outside the MinIO service, it intercepts the writes from the
client before objects are written to the object store. With this interception,
Aegis can scan the object and decide whether to allow the object to be written
to the store or to quarantine the object. The candidate design is shown in
figure \ref{fig:writeInterceptArch}.

\begin{wrapfigure}{r}{0.4\textwidth}
  \centering \includegraphics[scale=.3]{diagrams/write-intercept.png}
  \caption{Write Interception Architecture}
  \label{fig:writeInterceptArch}
\end{wrapfigure}

This candidate design has similar benefits as the second candidate design. It
offers the most protection against malicious files by never allowing either
unscanned or infected objects to be stored in the object store. However, it also
has similar drawbacks. This is because Aegis is still in sequence with MinIO
meaning that for optimal throughput, Aegis would need to match the performance
of MinIO.

Similar to the upload queue candidate design, this design also requires Aegis to
have a large cache to handle the largest files at maximum throughput. This cache
must also be large enough to handle the number of objects being put by MinIO
into the store. This issue cannot be mitigated without the risk of compromising
performance at increased loads.

However, this design does have an advantage over the second candidate design as
there is less responsibility placed on Aegis to be as failure tolerant. MinIO is
still directly responsible for accepting objects into the store and therefore is
still responsible for the recovery of the system in a failure scenario. This
allows the scope to focus on more related features to malware scanning.

% TODO Candidate Design 3: Encrypted???

\subsection*{Candidate Design 4 - Per Node}
\paragraph{}

% TODO Candidate Design 4: More?
The final candidate design distributes Aegis onto each node in the object store.
This means that each node has a local instance of Aegis that is responsible for
scanning objects before they are written to the store. The candidate design is
shown in figure \ref{fig:perNodeArch}.

\begin{wrapfigure}{r}{0.5\textwidth}
  \centering \includegraphics[scale=.3]{diagrams/per-node.png}
  \caption{Antivirus per Node Architecture}
  \label{fig:perNodeArch}
\end{wrapfigure}

This candidate design makes use of the distributed nature of MinIO to match the
demand when scaling out the system. As more nodes are added, more Aegis
instances are added to handle the increased scanning demand. This removes the
need for having a cache repository as Aegis already has access to the files that
need scanning. By removing this single point of failure, in theory, the system
only relies on the antivirus pod to be able to scale out on its own.

Independent scaling of the antivirus pod allow for efficient usage of available
hardware. A simple load based auto-scaler can be used to scale the number of
pods based on the current load. This allows for the system to flexible scale
with the demand of the system and to reduce usage of valuable resources, such as
power. There is also the opportunity to use intelligent scaling techniques to
predict the load on the system and prematurely scale the system to meet the
demand. For example, to scale the number of pods depending on the time of day or
the day of the week.

% TODO Candidate Design 4: Explain MinIO terms
The major drawback of this candidate design is that it replies on the ability to
scan whole files by only using data on a single node. In actual implementations,
MinIO makes use of erasure coding to add increased redundancy to the store
\citep{minio-erasure}. Erasure coding splits objects into multiple parts known
as blocks, and then calculates corresponding parity blocks. These data and
parity blocks are then distributed among all nodes in the system allowing for
on-the-fly data recovery even with the loss of multiple drives or nodes . This
means that the Aegis instance on each node only has access to the part available
on their node and therefore will not be able to reconstruct the whole file for
scanning. This makes this candidate design unsuitable for MinIO as it erasure
coding is one of its key features.

\subsection*{Selected Candidate Design}

Given the above evaluations of each candidate design, design one best meets the
requirements and constraints of the project. It makes the most use of the
existing features that MinIO provides in order to handle failure scenarios and
to scale out. This also means that this design has less critical responsibility
and will better fit the scope constraints allowing for more time to be spent on
supplementary features, such as testing, logging, and metric collection. Because
of this, the produced solution will be closer to production-ready than the other
candidate designs.

This candidate design keeps the user in control by giving them the ability to
store unscanned files / known malware without wasting resources on a scan.
Protection can be added per bucket therefore a user could have a known malware
bucket and a clean bucket within the same object store. This allows for the
system to be more flexible and to be able to handle more use cases. Candidate
Designs two and three would not be as able to handle this use case as they both
scan all objects before they are written to the store.

The size of the cache required is smaller than all other candidate designs as it
only needs to store the objects actively being scanned. This is in opposition to
upload queue and write interception candidate designs as they have to be
prepared to handle the full demand placed on the store. This makes candidate
design one the most lightweight of all the candidate designs which should lead
to a smaller resource footprint.

% TODO Selected Candidate Design: This is a bad ending :(
Overall, implementing candidate design one is the best option to create a
lightweight yet secure and scalable solution.

\section{Implementation}

\subsection*{Service Selection and Creation}

% TODO Implementation: Why did separate services need to be started?

Throughout the implementation, the development of the solution followed a
waterfall approach. The project's progress was determined by milestones outlined
in the project plan Gantt chart, which can be found in the appendix at figure
\ref{fig:gantt}. As this is a solo project with a limited timeline, the
waterfall methodology is a better fit than an agile methodology as agile caters
more to larger team with longer timelines where the requirements are less concrete.

Before beginning to implement the solution, it was necessary to
research, create, configure, and understand each external service that would be
used. Up to this point, the types of services needed were identified, but the
specific services to be used had not been determined. This section will describe
the process of selecting each service and then initially configuring them to
form a bare-bones proof of concept.

\subsubsection*{Object Store - MinIO}
\paragraph{}

The object store is the only service that did not require further research or
comparison as it is already the subject of this project. However, creating and
configuring a local instance of MinIO for development was necessary. MinIO
itself is available from various sources, including Docker, Homebrew, and the
MinIO website. Homebrew, a MacOS package manager, was chosen as it is the
easiest to install and update. The MinIO documentation was then used to create a
local instance of MinIO accessible through the web client
at \url{http://localhost:9000}. This allowed for the creation of buckets,
uploading objects to the store, and familiarization with MinIO's features.

% TODO Object Store: Include images

MinIO has integrated the ability to send notifications to event queues depending
on the operation performed on the store. This makes it quick and easy to set up
a locally running instance of an event queue to read messages sent by MinIO.
MinIO offers wide support for many different event queues, such as Kafka,
Webhook, Redis, PostgreSQL, and many more.

\subsubsection*{Event Queue}
\paragraph{}

Kafka is a popular and well-supported event queue that is used in many different
industries. It offers many features that make it a good choice for this project,
such as high-throughput, low latency, and open-source. Kafka is also available
from both Docker and Homebrew, making it easy to install and run locally. Most
modern event queues offer similar high throughput and low latency, but Kafka is
lighter in resource usage than other queues, such as RabbitMQ
\citep{kafka-rabbitmq}. This is beneficial as it will be easier to implement and
not overuse resources when it has a simple use case.

Kafka has a dependency on Zookeeper, which is a distributed coordination
service. Zookeeper must be installed as a separate service but is available from
the same sources as Kafka. Kafka can be run in Kafka Raft mode (KRaft), which
will eventually replace Zookeeper, but as of writing KRaft has not been fully
adopted yet \citep{kafka-raft}.

With Kafka and Zookeeper set up, the Kafka command line interface (CLI) was used
to start the service and create a topic. The MinIO documentation was then used
to configure MinIO to send all put notifications to the Kafka topic. The Kafka
CLI was used to read messages from the topic and see the messages sent by MinIO.
This demonstrated that the event queue is working and that MinIO is sending
messages to it whenever a put operation is performed. An example Kafka message
is available in the appendix in listing \ref{appendix:kafka-notif}.

% TODO Event Queue: Reference TODO Object Store: Include images?

My apologies for that, here is the complete version:

\subsubsection*{Antivirus}
\paragraph{}

The antivirus chosen needed to meet a list of requirements for it to be suitable
for use in this project. Firstly, it must be able to scale with the solution in
order to keep up with the demand placed on the system. Secondly, it must have a
CLI that the program can interact with in order to scan files. Finally, it must
be free or as inexpensive as possible to make it viable for commercial use. This
narrows down the available options to a few contenders.

Sophos is a popular antivirus that is used by many businesses and is available
for free for personal use. However, it is paid for commercial use, which makes
it less suitable for this project.

ClamAV, on the other hand, is completely free and open-source. It comes with a
scalable and multi-threaded daemon that can be accessed via CLI for
high-performance and on-demand file scanning \citep{clamav}. It is capable of
scanning many different file types, including archives and mail files. Built-in
is freshclam, a tool for automatically updating the virus database definitions.
The virus database itself is also open-source and is updated regularly by the
open source community. ClamAV has a docker image and is available from Homebrew.

ClamAV was chosen for this project because it is entirely free for commercial
use, open-source, and has a CLI that can be used to scan files. It is also very
well documented and has a large community of users.

ClamAV comes with a daemon, clamd, that can be run in the background and can be
accessed via a CLI using clamdscan, the clamd client. A configuration file is
needed to point clamdscan to the IP address that the daemon is running. In this
case, it is running locally on port 3310. Performing the clamdscan command and
providing a file will scan the file and return the result. An example of this is
available in the appendix in listing \ref{appendix:clamd-scan}.

% TODO Object Store: Include images?

\subsubsection*{Metric Collection}
\paragraph{}

As the project is expected to be as production-ready as possible, a system for
collecting metrics is needed. This will allow the system to monitor its activity
for easier maintenance and debugging. A few different options are available for
metric collection, such as Prometheus, InfluxDB, and Graphite.

% TODO Metric Collection: Reference why we need metrics

% TODO Metric Collection: Complete comparison

From this comparison, Prometheus was chosen as it is open-source, uses a pull
model with the option for a push gateway, and it does not require a distributed
system to run, unlike InfluxDB. Prometheus is available from both Docker and
Homebrew.

Prometheus is currently unusable as it is not being sent any metrics to collect.
However, the Prometheus server can still be launched, and the web interface can
be accessed on port 9090. In the meantime, the Prometheus documentation can be
used to create a configuration file defining the port and endpoint expected to
be exposing metrics to, in this case, port 2112 and /metrics.

% TODO Metric Collection: Reference Prometheus docs?

% TODO Object Store: Include images?

\subsubsection*{Audit Log Store}
\paragraph{}

Production-ready software should have a method to store logs for auditing and
analysis purposes. A central database can be used due to the expected low amount
of data needing to be written. The audit log's purpose is to store information
about the scans performed by the system, such as the time and result of the
scan, as well as the antivirus used. This allows the system to review previous
scans, which can aid in debugging and maintenance.

Various options exist for a database to store the audit logs, including
PostgreSQL, MySQL, MongoDB, and many more. PostgreSQL is considered the most
versatile of these options, as it is a relational database and can be used for
many different purposes. Since the use case is simple, maintaining a single
database service is the best option for maintainability and usability.
PostgreSQL is also available from both Docker and Homebrew.

% TODO Audit Log Store: Compare some more databases?

At this stage, PostgreSQL is not yet storing any data. However, the PostgreSQL
server can be launched, and the database accessed via the PostgreSQL shell
prompt (psql). A database and a user for the database can be created. The
database can then be exposed to port 5432 on the localhost, making it ready for
Aegis to use.

% TODO Audit Log Store: Reference why we need metrics TODO Object Store: Include
% images?

\subsubsection*{Data Visualisation}
\paragraph{}


% TODO Data Visualisation: Why we need data visualisation
Grafana

% TODO Data Visualisation: Compare other data visualisation tools - prometheus

Compare

% TODO Initial Service: Docker?? Or include that in each service
Docker

% TODO Object Store: Include images?


\subsection*{Aegis Module Design and Creation}
\paragraph{}

Now all of the dependencies have been downloaded, initialised and configured,
the Aegis Go module can be created. A module is a collection of packages

The most common language to use for micro-service based projects is GoLang. Go
is a compiled language that is statically typed and high concurrency support.
This makes it a perfect fit for this project to ensure that the system is as
performant as possible. MinIO is also written in Go should allow for easier
integration. Go has certain standards and guidelines for clean architecture that
should be followed to ensure high usability and efficiency.

Go introduces some specific language that are commonly used during the project.
A list of common terms and their definitions are available in table
\ref{table:go-terms}.

\begin{table}[H]
  \centering
  \begin{tabular}{|l|p{0.7\textwidth}|}
    \hline
    \textbf{Go Specific Term} & \textbf{Definition} \\
    \hline
    Go keyword & Launches a function as a goroutine, which is a lightweight thread managed by the Go runtime, enabling concurrent execution. \\
    \hline
    Select    & Waits on multiple communication operations, typically used with channels, to handle different cases depending on which operation is ready. \\
    \hline
    Switch    & Multi-way branching statement that allows conditional execution of code blocks based on the evaluation of an expression or comparisons. \\
    \hline
    Goroutine & Lightweight thread managed by Go runtime, allowing concurrent execution of functions without the overhead of managing traditional threads. \\
    \hline
    Channel   & Communication mechanism between goroutines, allowing them to synchronize and share data, providing a way to safely pass data between them. \\
    \hline
    Struct    & Composite data type for creating custom structures composed of different fields, useful for grouping related data together. \\
    \hline
    Package   & Collection of related Go source files organized under a specific namespace, allowing code organization, reuse, and dependency management. \\
    \hline
    Module    & Collection of related Go packages, providing versioning and dependency management, simplifying the process of building and sharing Go code. \\
    \hline
    Capitalised first letters & All functions, variables and types that are
                                intended to be exported outside of a package must be capitalised. \\
    \hline
    Defer & Defers the execution of a function until the surrounding function returns. \\
    \hline
  \end{tabular}
  \caption{Go Specific Terms and Their Definitions}
  \label{table:go-terms}
\end{table}

\subsubsection*{Project Structure}
\paragraph{}

In Go you should the separate the code into different packages, where each
package represents a distinct function of the system. This allows for easier
maintenance and debugging in the future. These packages are then grouped into
different directories depending on their intended scope and function. There are
three main directories that are used in GoLang projects, cmd, pkg and internal.
The cmd directory is used to store the main.go file which manages the workflow
and is the entry point of the program. The pkg directory is used to store all of
the packages that are intended to be used by external applications, in this case
our other services. The internal directory is used to store all of the packages
that are intended to be used by the application itself. This is where the
packages in the pkg directory will be consumed to perform the Aegis' core
actions as follows:

\begin{itemize}
  \item Listen for PUT events on the event queue
  \item Read the message from the event queue and extract the bucket and object
        path
  \item GET the object from the object store and store in cache
  \item Initiate a scan on the object using the antivirus software
  \item Collect the result of the scan and add tags to the object
  \item Collect metrics throughout the process
  \item Store the result of the scan in the audit log
  \item Expose metrics to Prometheus
\end{itemize}

From these requirements, the internal design of Aegis can be planned. We can
visualise this plan using various UML diagrams such as, class, sequence and flow
diagrams. Class diagrams are useful for understanding the relationships between
different classes - in this case packages - and what attributes and methods they
require \citep{class-diagrams}. It is worth noting that Go itself does not have
classes but instead uses structs which can be used to achieve the same effect.
Sequence diagrams are also valuable for understanding the timeline of the system
during the execution of a workflow \citep{uml-diagrams}. Flow diagrams are
useful for visualising the flow of processes and decisions during a workflow.
This class diagram is shown in the appendix at figure
\ref{appendix:class-diagram} along side with a both the sequence and flow
diagram at figures \ref{appendix:general-sequence} and
\ref{appendix:flow-diagram} respectively.

\begin{wrapfigure}{r}{0.4\textwidth}
  \centering \includegraphics[scale=.3]{diagrams/file-struct.png}
  \caption{Aegis' Initial File Structure}
  \label{fig:file-struct}
\end{wrapfigure}

From this diagram a file structure can be created inline with Go standards. Go
comes with a CLI tool used to initialise a new go module, which is a collection
of packages that are intended to be used together. This tool will create a go
mod file which is used to define the module and its dependencies. This tool will
also create a go sum file which is used to store the hashes of the dependencies
to ensure that the same version is used across all environments. Now go commands
can be used to install go dependencies and then download them to a local vendor
folder for use in building. Building the project is also done with the go CLI.
All binaries are stored within the build folder. The initial project structure
is shown by the figure \ref{fig:file-struct}.

\subsubsection*{Version Control}
\paragraph{}

Considering the size of this project, version control is necessary to ensure
maintainability and the availability of a backup. Git, in combination with
GitHub, was chosen for version control due to its widespread use, familiarity,
and cost-free nature. The Git CLI was used to initialize a new repository within
the root directory of the Go module. Additionally, a \texttt{.gitignore} file
was created to prevent unnecessary files from being tracked by Git, such as, the
vendor or build folder.

To maintain usability, commits to the remote GitHub repository will be made
after each significant change to the project, accompanied by relevant commit
messages. This approach facilitates easier tracking of changes for debugging and
maintenance purposes.

\subsubsection*{Structured Logging}
\paragraph{}

Structured logging is a method of logging that allows for easier parsing of the
log messages by adding structure to the message. This is done by adding key
value pairs containing relevant information to the log message. This enables for
easier filtering and searching of the logs. This is especially useful when using
a log aggregation tool such as Humio.

Go has many external modules that can handle this type of logging including Zap
by Uber. Zap is a very fast and efficient logging library that can be configured
to change the level of logging output, such as for info or debug useful for
production or development respectively \citep{zap-repo}. Zap also has the
ability to change between structured and unstructured logging depending on the
use case. Structured logging comes with the drawback of being less human
readable than unstructured logging and therefore Zap offers the ability to
change between the two \citep{zap-repo}.

% TODO Structure Logging: Humio Ref, Structured logging ref. Zap ref

As Zap is an external module, it must be added as a dependency and all code
contained in the pkg folder. A package called logger is created to encapsulate
all logging functionality. This package contains two go files, one for
interacting with the Zap and one for defining the structure of the log commands
inside of an interface. In Go, it is idiomatic to keep the interface as close to
the implementation as possible. Keeping the interface in the same package as the
implementation allows for easier mocking of the package when testing. This
package goes against this by containing the interface within the repository
because multiple packages will need to use the same interface when interacting
with them. This reduces code duplication as we don't have to define the
interface in each package that uses it, which in this case would be every
package. The code is shown in figure \ref{fig:logger-code} and the interface is
shown in figure \ref{fig:logger-interface}.

% TODO Structure Logging: Include code extracts TODO Structure Logging: Example
% of using the interface?

\subsubsection*{Configuration}
\paragraph{}

As Aegis is a micro-service, it is important that it is configurable to allow
for easy deployment to different environments. This is done by using a
configuration file in tandem with Viper, a Go module that can read in the
configuration file.

This configuration file contains all of the values that are likely to change
between environments. This includes values such as the endpoints, ports and
credentials of external services, logging options and database names. This
configuration file will be a dotenv file which is a key value store of
environment variables. This is beneficial as Viper has the ability automatically
override the config file with environment variables if the same key is found.
This allows for easier configuration of the application when it is built locally
or deployed in a kubernetes cluster. This gives the user the ability to tailor
the deployment to their needs or existing implementation.

Much like the logger package, the configuration package contains two go files. A
repository file that defines the interface and a config file that contains the
Viper configuration.

% TODO Configuration: Figure of dotenv and explain mapstructure

% TODO Configuration: Viper repository reference

Adding both the configuration and structured logging as the first packages
reduces the need for refactoring in the future. No hard coded values are needed
for initial development as all values can be stored in the configuration file
from the start.

\subsubsection*{Makefile}
\paragraph{}

A Makefile is a file that contains a set of instructions that can be run from
the command line. These instructions are used to automate processes such as
building, testing and deploying. Through the project, longer workflows will be
automated and put into the Makefile to reduce the amount of time spent on
running commands. Makefiles also simplify the usage of program by users by
encapsulating complex commands into an explicit action the user can understand.
Makefile commands can also be used by the Dockerfile when building the
application in a container.

% The Makefile is available for viewing in the repository

% TODO Makefile: Link to GitHub


\subsection*{External Package Integration}
\paragraph{}

With the dependency services still running, the next step is to integrate them
into the Aegis application using Go. This is done by creating a new package for
each of the services within the pkg folder. Each package will contain a go file
for each of the services that will later be consumed by Aegis' internal
workflow.

One significant advantage of separating external services from internal
implementation is that it allows for a higher level of abstraction from the
services used. This abstraction provides flexibility in the use of multiple
external services that fulfill the same purpose, such as multiple antivirus
scanners. The creation of external packages allows for the use of a single
internal implementation for all of these services. This reduces the need for
extensive changes in the future, making the application more maintainable and
scalable.

\subsubsection*{MinIO}
\paragraph{}

The initial external service to be incorporated is MinIO. The MinIO package
manages interactions with the MinIO service. For this project, the required
operations include getting, putting, and removing objects, as well as getting
and putting tags. MinIO provides a Go Software Development Kit (SDK) that
already supports these operations in Go \citep{minio-go-repo}. The SDK
simplifies the complexities of making requests and offers straightforward
methods for operations such as putting and getting, as well as accessing type
definitions like tags.

Once the MinIO SDK is imported, a new MinIO client object is generated to
communicate with the MinIO service. The \texttt{CreateMinio} function accepts
essential connection parameters, such as a context, an endpoint, access and
secret keys, and an SSL usage flag, and initializes the MinIO client using the
\texttt{minio.New} function. This client object is then incorporated into a
custom \texttt{Minio} struct, along with a logger. Various methods are
implemented for the Minio struct to execute different object storage operations:

\begin{itemize}
  \item \texttt{GetObject}: Retrieves an object from a specified bucket and
        returns its data as a byte slice.
  \item \texttt{PutObject}: Takes in a byte stream and uploads it to a specified
        bucket with a specified object name.
  \item \texttt{RemoveObject}: Removes a specified object from a specified
        bucket.
  \item \texttt{GetObjectTagging}: Fetches the tags associated with an object
        and returns them as a map of key-value pairs.
  \item \texttt{PutObjectTagging}: Replaces the existing tags of an object with
        a new set of tags provided as a map of key-value pairs.
  \item \texttt{AddObjectTagging}: Adds new tags to an object by first fetching
        the existing tags, updating them with the new key-value pairs, and then
        setting the updated tags back to the object. Necessary for not
        overriding existing object tags that may exist.
\end{itemize}

All of these methods take in a context which is used in the shutdown process to
close the connection to the MinIO service. This context is passed in during the
creation of the MinIO struct so that all methods have access.

\subsubsection*{Kafka}
\paragraph{}

The next external service to be integrated is Kafka. The Kafka package will
handle the interactions with the Kafka service. For this project, the operations
needed are to consume messages from a specified topic. Kafka provides a Go
library, \textit{kafka-go}, which simplifies the consumption of messages in a Go
application \citep{kafka-go-repo}.

The package imports necessary dependencies and creates a custom
\texttt{KafkaConsumer} struct, which embeds a \texttt{kafka.Reader} object and a
logger. The \texttt{CreateKafkaConsumer} function initializes a new
\texttt{KafkaConsumer} instance by taking connection parameters such as the list
of brokers, the topic to be consumed, a group ID, and a maximum number of bytes
per message.

\begin{itemize}
  \item \texttt{ReadMessage}: Uses the Kafka library to halt until a message is
        received from the specified topic. It then decodes it using the
        \texttt{decodeMessage} function, and returns the bucket name and object
        key.
  \item \texttt{decodeMessage}: Decodes a Kafka message by unmarshalling its
        JSON payload and extracting the bucket name and object key. If the
        message event is \textit{s3:ObjectCreated:PutTagging}, it returns empty
        strings, as this event does not require processing.
\end{itemize}

During the shutdown process, the \texttt{ReadMessage} function is halted by
closing the context passed in. This closes the connection to the Kafka service
and therefore stops the reading of any new Kafka messages, leaving unprocessed
messages in the event queue.

\subsubsection*{ClamAV}
\paragraph{}

ClamAV is another external service to be integrated into the project. The
primary operation needed for this project is scanning a file and returning the
scan results. The ClamAV daemon can be interacted with through the command-line
interface (CLI) using the \texttt{clamdscan} command \citep{clamav-repo}.

Initially, a \texttt{ClamAVScanner} struct is created, embedding a logger. The
\texttt{CreateClamAV} function initializes a new \texttt{ClamAVScanner}
instance. Methods for the \texttt{ClamAVScanner} struct are implemented to
perform various file scanning operations:

\begin{itemize}
  \item \texttt{ScanFile}: Accepts a file path as an argument and scans the file
        using the built-in Go exec library to run the \texttt{clamdscan} command
        with the \texttt{--config} flag set to use a custom configuration file
        located at \texttt{clamav.conf}. The exec library enables the execution
        of external commands, providing the ability to interact with the ClamAV
        antivirus daemon. A process attribute is added, which starts the process
        in a different process group than the main execution. This approach aids
        in achieving a graceful shutdown later on, as calling a system interrupt
        terminates the entire process group \citep{process-groups}. As a result,
        \texttt{ScanFile} can continue executing after the shutdown, ensuring
        that no scans are interrupted. The method returns false if the file is
        clean, true if infected, and the type of malware detected. If any errors
        occur during the execution, it returns true (infected) along with an
        error message, ensuring that the worst-case scenario is assumed when it
        comes to security.
  \item \texttt{findVirusType}: Takes the output from the \texttt{clamdscan}
        command, extracts the virus type using regular expressions, and returns
        it as a string.
  \item \texttt{GetName}: Returns the name of the antivirus engine, in this
        case, "clamav". The name must be accessed through a method, as ClamAV
        will implement an interface that does not have access to any attributes.
\end{itemize}

The internal scanner package can now use the ClamAV package as one of the
antivirus engines to scan files it receives.

\subsubsection*{Prometheus}
\paragraph{}

The Prometheus package is in charge of creating and managing an HTTP server that
exports metrics from Aegis. It does this by providing a plaintext response
containing the metrics in the Prometheus exposition format. The exposed endpoint
is then used by the Prometheus server to collect metrics from Aegis.

\begin{itemize}
  \item \texttt{CreatePrometheusServer}: Initializes a new Prometheus exporter
        by setting up a new HTTP server with the specified endpoint and path.
        The Prometheus handler, provided by the \texttt{promhttp.Handler()}
        function from the Prometheus Go client library, is linked to the given
        path, which serves the plaintext. Read and write timeouts for the HTTP
        server are established using constants since these values are not
        expected to be configurable.
  \item \texttt{Start}: Initiates the Prometheus server by calling the
        \texttt{ListenAndServe()} method on the HTTP server. An example of the
        generated plaintext output can be found in the appendix at listing
        \ref{appendix:example-exposed-metrics}.
  \item \texttt{Stop}: Handles the graceful shutdown of the Prometheus server by
        invoking the \texttt{Shutdown} method to close the HTTP server.
\end{itemize}
% TODO Prometheus: Maybe include diagram? TODO Prometheus: Is implement used
% correctly?

\subsubsection*{PostgreSQL}
\paragraph{}

The package imports necessary dependencies and creates a custom
\texttt{PostgresqlDB} struct, which embeds a \texttt{pgxpool.Pool} object and a
logger. The \texttt{CreatePostgresqlDB} function initializes a new
\texttt{PostgresqlDB} instance by taking connection parameters such as the user,
password, endpoint, and database name. It also returns a \texttt{CloseFunc}
function to close the connection when needed.

\begin{itemize}
  \item \texttt{CreatePostgresqlDB}: Is responsible for establishing a
        connection to the PostgreSQL database and returning a
        \texttt{PostgresqlDB} instance. The function takes in connection
        parameters such as the user, password, endpoint and database name.
        Additionally, it returns a \texttt{CloseFunc} function to facilitate a
        graceful shutdown of the connection pool when necessary. Instead of
        connecting straight to the database, the function uses a connection pool
        to manage connections. This allows multiple concurrent clients to
        perform operations on the database without having to wait for other
        clients to finish their transactions. In this case, when multiple files
        are being scanned at the same time and the results are being saved to
        the database, the connection pool ensures that a database connection is
        always available.
  \item \texttt{CreateTable}: Uses the connection pool to create a new table
        with the specified name if it does not exist. The table schema includes
        columns for ID, ObjectKey, BucketName, Result, Antivirus, Timestamp, and
        VirusType. The SQL query used to execute this operation is available in
        the appendix at listing \ref{appendix:create-table-query}.
  \item \texttt{Insert}: Uses the connection pool to insert a record into the
        specified table with values for ObjectKey, BucketName, Result,
        Antivirus, Timestamp, and VirusType. The SQL query used to insert is
        available in the appendix at listing \ref{appendix:insert-query}.
\end{itemize}

The PostgreSQL instance is provided with a context to close the connection to
the database when the application is shutting down.

\subsection*{Aegis' Internal Workflow}
\paragraph{}
% TODO Aegis' Internal Workflow: Describe what internal means

\subsubsection*{Metrics and Collectors}
\paragraph{}

The internal metrics collection comprises two primary components: the metric
manager and various metric collectors specific to each package. The metric
manager is responsible for managing interactions with Prometheus, which includes
executing the \texttt{Start} and \texttt{Stop} methods for handling the starting
and graceful shutdown of Prometheus respectively.

Each package contains a metric collector in a file named \texttt{metrics.go}.
These collectors define the available metrics that can be collected and exported
by the respective packages. The \texttt{promauto} library facilitates the
creation of a global registry when the metric manager is initialized. This
registry is accessed by all metric collectors to record the metrics they collect
and is also utilized by the Prometheus exporter for publishing these metrics.

\subsubsection*{Object}
\paragraph{}

The object package presents the \texttt{Object} struct as the internal
representation of an object within the object store. It includes all methods and
attributes related to an object, such as the object key and bucket name.
Operations involving an object are performed within the object instance itself.

Since the object represents a concrete entity, there is no need for an interface
when using it. This design choice allows the object to have attributes that can
be accessed directly, without the need for getter functions.

The \texttt{CreateObject} function enables the creation of a new \texttt{Object}
instance, given a specified object key and bucket name.

The \texttt{SetCachePath} method defines the cache path for an object by
concatenating the cache path, bucket name, and object key, separated by slashes.
This method is called when an update to the cache path is needed.

The \texttt{SaveByteStreamToFile} method stores an object's byte stream in a
file. First, it checks if the path attribute is empty since, by default, no path
is provided. It returns an error if this is the case, as other types of scanners
may not always require this information to perform a scan. Next, it ensures that
the file's parent directory exists, creating it if necessary. Lastly, the method
writes the byte stream to the file using Go's built-in IO writer.

The \texttt{RemoveFileFromCache} method is responsible for deleting an object
file from the cache. It tries to remove the file specified by the object's path
attribute. If the removal is unsuccessful, it logs an error message and returns
the error.
% TODO Object: Why object KEY?

\subsubsection*{Events Manager}
\paragraph{}
The events package includes the event manager, which is responsible for reading
messages from the event queue and forwarding scan requests to the scanner. The
\texttt{Kafka} interface provides methods for reading messages from the Kafka
queue and closing the connection. The \texttt{EventsManager} struct consists of
four fields: a \texttt{logger}, a \texttt{kafka} instance for interacting with
Kafka, a \texttt{scanChan} channel to forward scan requests and an
\texttt{eventsCollector} for gathering metrics.

The \texttt{CreateEventsManager} function creates a new \texttt{EventsManager}
instance, accepting the necessary arguments. The \texttt{Start} method of the
\texttt{EventsManager} takes in a context and then enters a loop that uses a
switch statement to first check if the context has been canceled. If it has, it
closes the \texttt{scanChan} channel, closes the Kafka connection and returns.
Otherwise, it invokes the \texttt{ReadMessage} method of the \texttt{kafka}
instance to read a message from the Kafka queue. Upon confirming that there is
no error and the message is not nil, it increments the \texttt{eventsCollector}
counter and creates a new \texttt{object.Object} instance with the received
bucket name and object key. It then forwards the object to the \texttt{scanChan}
channel for scanning.

Since the event manager runs within a goroutine, if an error occurs, it sends
the error to the provided \texttt{errChan} channel.

\subsubsection*{Object Store}
\paragraph{}
In the object store package, several structs and interfaces are defined to
handle object storage operations. The \texttt{Minio} interface contains the
abstract object store operations, such as; get, put and remove objects, as well
as get and put object tags. These are also reflected by the
\texttt{ObjectStoreCollector} in the form of metric counters that track the
number of each operation performed.

Once the object store is created by the \texttt{CreateObjectStore} function, it
can be used by the rest of the application to perform object storage operations.
In addition to the standard object storage operations, two more operations are
added to the object store: \texttt{MoveObject} and \texttt{AddObjectTagging}.
These both combine multiple standard operations into one as follows:

\begin{itemize}
  \item \texttt{MoveObject}: Retrieves an object from the source bucket, puts it
        into the destination bucket, and removes it from the source bucket.
  \item \texttt{AddObjectTagging}: Retrieves the object tags from the source
        bucket, adds the new tags to the existing ones, and puts the combined
        object tags onto the object.
\end{itemize}

\subsubsection*{Object Scanner}
\paragraph{}
The scanner package provides the functionality for multiple workflows when it
comes to scanning an object. In this instance, an object scanners downloads from
the object store, performs a scan with its antivirus engines, and then passes
the result to a cleaner which will execute the cleanup policy. Having the
ability to use multiple types of scanners allows for flexibility in the system
as in the future, the workflow for scanning an object might change. For example,
if one of the antivirus engines could require the hash of the file. In this
case, another scanner called \texttt{HashScanner} could be created to handle
this alternate workflow. For this project, the \texttt{ObjectScanner} will be
the only type of scanner implemented.

The \texttt{CreateObjectScanner} function creates a new \texttt{ObjectScanner} instance with the following arguments:

\medskip
\begin{itemize}
  \item \texttt{logger}: A logger instance for logging messages.
  \item \texttt{objectStore}: An object store instance for downloading objects.
  \item \texttt{antiviruses}: An array of antivirus instances for scanning
        objects.
  \item \texttt{cleaner}: A cleaner instance for cleaning up objects.
  \item \texttt{auditLogger}: An audit logger instance for logging scan results.
  \item \texttt{scanCollector}: A scan collector instance for collecting
        metrics.
  \item Various configuration values, such as, \texttt{removeAfterScan},
        \texttt{datetimeFormat}, and \texttt{cachePath}.
\end{itemize}

All instances passed to the \texttt{CreateObjectScanner} function are interfaced
to both allow for future mocking and to allow for other abstract implementations
of the interfaces.

The \texttt{ScanObject} method handles the workflow for downloading and scanning
an object. It takes in an \texttt{object.Object} instance, which it fetches from
the object store, and an \texttt{errChan} for returning errors. It then sets the
object cache path by calling \texttt{SetCachePath} on the object. With this set,
the scanner can perform a \texttt{GetObject} on the object store to retrieve the
byte stream of the object and call \texttt{SaveByteStreamToCache} with the byte
stream to save it to the cache.

The scanner can now perform a scan on the object by calling \texttt{Scan}, with
the cache location, on every antivirus engine. If any of the antivirus engines
detect the object as infected, then using the assume the worst mentality, the
file is deemed infected. The object is then passed to the cleaner to execute the
cleanup policy. During this execution various metrics are being collected by the
\texttt{scanCollector} about the scan, such as, the number of clean or infected
files, total files scanned and total errors encountered. In addition audit logs
are also generated by the \texttt{auditLogger} for each scan by each antivirus,
recording the object key, bucket name, antivirus name, scan result, timestamp
and if infected, the virus type.

After performing the scan and dealing with the results, if the
\texttt{removeAfterScan} flag is set to true, the object is removed from the
cache after scanning.

The \texttt{ObjectScanner} is will be run within a goroutine, if an error is
encountered during the scan, it will be sent to the provided \texttt{errChan}.

\subsubsection*{Cleanup Policies}
\paragraph{}

% TODO Cleanup Policies: Read again

As mentioned in the previous section, the \texttt{ObjectScanner} passes the
object to the cleaner to execute the cleanup policy. The cleaner package defines
how to react given a clean or infected result from the antivirus engines.
Multiple policies are available in the \texttt{config.env} file to give the user
flexibility in how they want to deal with infected objects. These policies
include:

\begin{itemize}
  \item \texttt{Tag}: Adds a tag to the object in the object store based on the
        scan result.
  \item \texttt{Remove}: Removes the object from the object store if it is
        deemed infected
  \item \texttt{Quarantine}: Moves the object to a quarantine bucket if it is
        deemed infected.
\end{itemize}

In \texttt{CreateCleaner} function, a new \texttt{Cleaner} instance is created
with a logger, object store, metrics and audit loggers, and various
configuration parameters such as the cleanup policy and quarantine bucket.

The \texttt{Cleanup} method is called by the \texttt{ObjectScanner} where is
passes the object after it has been scanned. This method uses a switch
statement, shown in figure \ref{fig:cleaner-switch}, to determine which policy
to implement and executes the appropriate cleanup method. If no policy is
specified, the switch statement has a default cause of logging that it will do
nothing, in the case that the user only wants the audit logs. However with a
given cleanup policy, the corresponding cleanup method is called. Each of these
use the object store and object store to perform the cleanup.

\begin{figure}[H]
\begin{lstlisting}[language=Go]
    switch c.cleanupPolicy {
    case "tag":
      err = c.TagInfected(object, result, scanTime)
    case "remove":
      err = c.RemoveInfected(object, result, scanTime)
    case "quarantine":
      err = c.QuarantineInfected(object, result, scanTime)
    default:
      c.logger.Warnln("No cleanup policy found")
    }
\end{lstlisting}
  \caption{Cleanup policy switch statement}
  \label{fig:cleaner-switch}
\end{figure}

\subsubsection*{Dispatcher}
\paragraph{}
% TODO Dispatcher: Is implement correct?

The \texttt{dispatcher} package is responsible for managing the scanning of
objects using multiple scanners concurrently. It defines a \texttt{Scanner}
interface with a single method, \texttt{ScanObject}, that will be implemented by
one of the available scanners. The \texttt{Dispatcher} struct contains three
fields: a \texttt{logger} for logging messages, a \texttt{scanChan} channel for
receiving object scan requests, with a \texttt{scanners} slice to hold the
available scanners.

The \texttt{CreateDispatcher} initialises a new \texttt{Dispatcher} instance
with the required fields. The \texttt{Start} method enters into a loop where it
ranges over the \texttt{scanChan} channel. If the channel is empty, the loop
will block until a new object is sent through the channel. If the channel has an
object, the dispatcher will spawn a new goroutine to handle the scan. However,
if the channel is closed, the loop will continue to process all objects in the
channel and then exit \citep{go-channel-ranges}. This is because when channels
are closed, no more values can be sent to them, but the values that have already
been sent can still be received \citep{go-closing-channels}. This is shown in
the provided dispatcher loop code in figure \ref{fig:dispatcher-loop}.

\begin{figure}[H]
\begin{lstlisting}[language=Go]
func (d *Dispatcher) Start(errChan chan error, done chan struct{}) {
	var wg sync.WaitGroup

	for request := range d.scanChan {
		for _, scanner := range d.scanners {
			wg.Add(1)
			go func(req *object.Object, sc Scanner) {
				defer wg.Done()
				sc.ScanObject(req, errChan)
			}(request, scanner)
		}
	}
	wg.Wait()
	done <- struct{}{} //Send empty done message
}
\end{lstlisting}
  \caption{Dispatcher loop}
  \label{fig:dispatcher-loop}
\end{figure}

When the program receives a termination signal, there should be no loss of
information about incoming scan requests. This is a security risk as it could
lead to objects not being scanned. To prevent this, the dispatcher uses a
\texttt{sync.WaitGroup} to wait for all goroutines to finish before exiting the
program. This is done by calling \texttt{wg.Add(1)} before starting a new
anonymous goroutine to increment an active goroutine counter, and using
\texttt{defer wg.Done()} when the goroutine has finished to decrement the
counter. The \texttt{wg.Wait()} call will block until the counter is zero,
meaning all goroutines will have finishes processing \citep{go-waitgroups}. This
nsures that all scans that are currently being processed since

texttt{scanChan} was close will be completed before the program exits.

\subsubsection*{Main}
\paragraph{}


The main package orchestrates the top-level workflow that Aegis executes

hroughout its operation. The entry point of the program is the \texttt{main}

unction, which performs one task - calling the \texttt{run} function and

xiting the program based on its return value. This design choice enhances

xtensibility and testability since alternate workflows can be implemented while

aintaining a single entry point. Furthermore, the \texttt{run} function can be

ested without running the entire program \citep{go-tiny-abstraction}. The

texttt{run} function serves as a abstraction from \texttt{main}, as it

ncompasses Aegis' main workflow.

The \texttt{run} function is divided into three distinct sections:

nitialization and configuration, the main loop, and cleanup. The initialization

nd configuration section is responsible for initializing all the components

egis requires and configuring them with the values provided by the

onfiguration package.

\medskip
\begin{itemize}
  \item The configuration is loaded from \texttt{config.env}, and the logger is

        reated. An initial context is also created and passed to everything

        hat requires it, apart from the event system.
  \item The metric manager and various metric collectors are created, with the

        etric manager taking in the Prometheus exporter.
  \item The audit logger is implemented by the PostgreSQL database.
  \item The object store is implemented by the Minio client.
  \item The event system is implemented by the Kafka consumer with the

        texttt{scanChan} passed in as well.
  \item The scanning workflow is created. This includes creating the antivirus

        ngines, in this case ClamAV, creating the cleaner with the configured

        olicy and passing both of them to the scanners, namely the object

        canner. The scanners are then passed to the dispatcher alongside the

        texttt{scanChan}.
\end{itemize}
\bigskip

% TODO Main: Parameter tree?

% ---- Main loop -----

The main loop is the continuous execution of the goroutines that handle Aegis'

synchronous operations. These are the event manager, dispatcher, and metric

anager. The goroutines are started using the \texttt{go} keyword, which spawns

 new goroutine to execute the functions in a separate thread.

An additional context is created and parsed into the \texttt{Start} method of

he \texttt{eventManager}. Multiple channels are then created to handle errors,

hutdown command, and the shutdown complete with \texttt{errChan},

texttt{shutdownChan} and \texttt{done} respectively. The \texttt{Start} methods

f the \texttt{eventManager}, \texttt{dispatcher} and \texttt{metricManager} are

alled to begin the main workflow of the program.

An error channel (\texttt{errChan}) is created to handle errors generated by the

vent manager and metric manager goroutines.


% ---- Shutdown -----

Finally, the shutdown section ensures a smooth termination when the program

eceives an interrupt signal or encounters errors from the goroutines. The

hutdown sequence is vital as it allows Aegis to maintain the progress of

rocessed objects when receiving messages, enabling it to resume scanning

bjects from where it left off upon restart. The shutdown sequence unfolds as

ollows:

When an interrupt signal or an error from any goroutine is received, Aegis

tarts its graceful shutdown sequence. A select statement is used to wait for a

essage from either the \texttt{errChan} or \texttt{shutdownChan} channels. If

ny of these channels receive a message, a message or error is logged, and the

hutdown sequence begins by canceling the context passed to the event manager.

his action halts the event manager from consuming messages from the Kafka

onsumer and subsequently closes the \texttt{scanChan} channel. As a result,

ncoming notifications remain in the Kafka queue and can be consumed by Aegis

pon restart leading to no scans lost.

The code then waits for the \texttt{done} channel to send a message, signaling

hat the goroutines have completed processing the remaining objects in the

texttt{scanChan} channel. Once this process is finished, the program stops the

rometheus metric exporter and exits with a status code of 0, indicating a

uccessful operation. To better indicate the graceful shutdown process, a

equence diagram is available in the appendix at figure

ref{appendix:shutdown-sequence}.

\subsection*{Testing}

\subsubsection*{Mocking}
\paragraph{}

\subsubsection*{Unit Tests}
\paragraph{}

\subsection*{Kubernetes Deployment}
\paragraph{}

To make the solution as production-ready as possible, deployment to a Kubernetes

s required.

\subsubsection*{Kubernetes}
\paragraph{}
Kubernetes (K8s) is an open-source container orchestration system for automating

eployment, scaling, and management of containerised applications \citep{k8s}.

\subsubsection*{Docker}
\paragraph{}
Docker is


\subsubsection*{Aegis Containerisation}
\paragraph{}


\subsubsection*{Helm}
\paragraph{}

\subsubsection*{K3d}
\paragraph{}

\section{Results and Evaluation}
% Evaluate against specification Compare to MinIO Find figures like average scan

% ompare local vs cluster
\subsection*{}

\section{Product Issues / Future work} % What you would do next time
\subsection*{}

\section{Conclusions}
\subsection*{}

\section{Reflection on Learning} % Fuck knows
\subsection*{}

\section{Appendix}
% TODO Appendix: Add figures and tables to appendix

\begin{figure}[H]
  \centering \includegraphics[scale=.38]{diagrams/class-diagram.png}
  \caption{Plan of Aegis' Internal Packages}
  \label{appendix:class-diagram}
\end{figure}

\begin{figure}[H]
  \centering \includegraphics[scale=0.53]{diagrams/general-sequence.png}
  \caption{Aegis General Sequence Diagram}
  \label{appendix:general-sequence}
\end{figure}

\begin{figure}[H]
  \centering \includegraphics[scale=0.5]{diagrams/flow-diagram.png}
  \caption{Aegis Scan Request Flow Diagram}
  \label{appendix:flow-diagram}
\end{figure}

\begin{figure}[H]
  \centering \includegraphics[scale=0.31]{diagrams/shutdown-sequence.png}
  \caption{Aegis Shutdown Sequence Diagram}
  \label{appendix:shutdown-sequence}
\end{figure}

\lstinputlisting[caption={Example Kafka Notification}, label=kafka-notif,

umbers=left]{assets/example-notif.json}

\bigskip \lstinputlisting[caption={Example Kafka Notification},

abel=appendix:clamd-scan]{assets/example-clamd-scan.txt}

\bigskip \lstinputlisting[caption={Example Exposed Metrics},

abel=appendix:example-exposed-metrics]{assets/example-exposed-metrics.txt}

\bigskip \lstinputlisting[language=SQL, caption={Create Table SQL Query},

abel=appendix:create-table-query]{assets/create-table-query.txt}

\bigskip \lstinputlisting[language=SQL, caption={Insert into Table SQL Query},

abel=appendix:insert-query]{assets/insert-query.txt}

\bigskip

\bibliographystyle{cardiff} \bibliography{references}

\end{document}

% TEMPLATED
% \begin{figure}[!ht]
%   \centering \includegraphics[scale=.55]{assets/extracred}
%   \caption{Bouncing and pitching motion as a function of time}
%   \label{fig:bounceAndPitch}
% \end{figure}

% \begin{table}[!ht]
%   \begin{center}
%     \caption{Calculated values}
%     \label{tab:calculated}
%     \begin{tabular}{|c|c|}
%       \hline
%     \end{tabular}
%   \end{center}
% \end{table}

% Appendix \onecolumn \textwidth=456pt \paperwidth=577pt \hoffset=-30pt
% \newpage
% \newpage
% \clearpage


% \pagestyle{headings}
% \section{Appendix A}
% \centering Heading \footnotesize

% \normalsize

% \csvautolongtable{assets/vibProj.csv}
%
